@book{tukey77,
  added-at = {2009-10-28T04:42:52.000+0100},
  author = {Tukey, John W.},
  biburl = {https://www.bibsonomy.org/bibtex/2e0c6e346a4594eb8f0bc71a6e65366fb/jwbowers},
  citeulike-article-id = {107137},
  date-added = {2007-09-03 22:45:16 -0500},
  date-modified = {2007-09-03 22:45:16 -0500},
  interhash = {69fff218408c5eda560e267ff0099dec},
  intrahash = {e0c6e346a4594eb8f0bc71a6e65366fb},
  keywords = {eda statistics},
  publisher = {Addison-Wesley},
  timestamp = {2009-10-28T04:43:20.000+0100},
  title = {Exploratory Data Analysis},
  year = 1977
}

@article{hubert2008adjusted,
  title={An adjusted boxplot for skewed distributions},
  author={Hubert, Mia and Vandervieren, Ellen},
  journal={Computational statistics \& data analysis},
  volume={52},
  number={12},
  pages={5186--5201},
  year={2008},
  publisher={Elsevier}
}

@inproceedings{breunig2000lof,
  title={LOF: identifying density-based local outliers},
  author={Breunig, Markus M and Kriegel, Hans-Peter and Ng, Raymond T and Sander, J{\"o}rg},
  booktitle={ACM sigmod record},
  volume={29},
  number={2},
  pages={93--104},
  year={2000},
  organization={ACM}
}

@article{elseberg2012comparison,
  title={Comparison of nearest-neighbor-search strategies and implementations for efficient shape registration},
  author={Elseberg, Jan and Magnenat, St{\'e}phane and Siegwart, Roland and N{\"u}chter, Andreas},
  journal={Journal of Software Engineering for Robotics},
  volume={3},
  number={1},
  pages={2--12},
  year={2012}
}

@article{todorov2009object,
  title={An object-oriented framework for robust multivariate analysis},
  author={Todorov, Valentin and Filzmoser, Peter and others},
  year={2009},
  publisher={Citeseer}
}

@article{gnanadesikan1972robust,
  title={Robust estimates, residuals, and outlier detection with multiresponse data},
  author={Gnanadesikan, Ramanathan and Kettenring, John R},
  journal={Biometrics},
  pages={81--124},
  year={1972},
  publisher={JSTOR}
}

@article{yohai1988high,
  title={High breakdown-point estimates of regression by means of the minimization of an efficient scale},
  author={Yohai, Victor J and Zamar, Ruben H},
  journal={Journal of the American statistical association},
  volume={83},
  number={402},
  pages={406--413},
  year={1988},
  publisher={Taylor \& Francis}
}

@article{maronna2002robust,
  title={Robust estimates of location and dispersion for high-dimensional datasets},
  author={Maronna, Ricardo A and Zamar, Ruben H},
  journal={Technometrics},
  volume={44},
  number={4},
  pages={307--317},
  year={2002},
  publisher={Taylor \& Francis}
}

@article{stuart1954,
 ISSN = {00359254, 14679876},
 URL = {http://www.jstor.org/stable/2985442},
 abstract = {In using the goodness-of-fit test should a very small value of Ï‡2 occasion a rejection of the null hypothesis? Mr Stuart presents the conflicting views on this question and answers it: No.},
 author = {Alan Stuart},
 journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
 number = {1},
 pages = {29-32},
 publisher = {[Wiley, Royal Statistical Society]},
 title = {Too Good to be True?},
 volume = {3},
 year = {1954}
}


@book{Zheng2012,
address = {Boston, MA},
author = {Zheng, Gang and Yang, Yaning and Zhu, Xiaofeng and Elston, Robert C.},
doi = {10.1007/978-1-4614-2245-7},
isbn = {978-1-4614-2244-0},
publisher = {Springer US},
series = {Statistics for Biology and Health},
title = {{Analysis of Genetic Association Studies}},
url = {http://link.springer.com/10.1007/978-1-4614-2245-7},
year = {2012}
}

@article{Kane2013,
abstract = {This paper presents two complementary statistical computing frameworks that address challenges in parallel processing and the analysis of massive data. First, the foreach package allows users of the R programming environment to define parallel loops that may be run sequentially on a single machine, in parallel on a symmetric multiprocessing (SMP) machine, or in cluster environments without platformspecific code. Second, the bigmemory package implements memoryand filemapped data structures that provide (a) access to arbitrarily large data while retaining a look and feel that is familiar to R users and (b) data structures that are shared across processor cores in order to support efficient parallel computing techniques. Although these packages may be used independently, this paper shows how they can be used in combination to address challenges that have effectively been beyond the reach of researchers who lack specialized software development skills or expensive hardware.},
author = {Kane, Michael J and Emerson, John W and Weston, Stephen},
doi = {10.18637/jss.v055.i14},
file = {:home/privef/Bureau/thesis-celiac/articles/Kane, Emerson, Weston - 2013 - Scalable Strategies for Computing with Massive Data.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
number = {14},
pages = {1--19},
title = {{Scalable Strategies for Computing with Massive Data}},
url = {http://www.jstatsoft.org/v55/i14/},
volume = {55},
year = {2013}
}

@article{Wickham2011,
abstract = {Many data analysis problems involve the application of a split-apply-combine strategy, where you break up a big problem into manageable pieces, operate on each piece inde- pendently and then put all the pieces back together. This insight gives rise to a new R package that allows you to smoothly apply this strategy, without having to worry about the type of structure in which your data is stored. The paper includes two case studies showing how these insights make it easier to work with batting records for veteran baseball players and a large 3d array of spatio-temporal ozone measurements. Keywords:},
author = {Wickham, Hadley},
doi = {10.1039/np9971400083},
issn = {0265-0568, 1460-4752},
journal = {Journal of Statistical Software},
keywords = {apply,data analysis,r,split},
number = {1},
pages = {1--29},
title = {{The Split-Apply-Combine Strategy for Data Analysis. Journal of Statistical Software}},
url = {http://www.jstatsoft.org/v40/i01/},
volume = {40},
year = {2011}
}

@article{Tibshirani2012,
abstract = {We consider rules for discarding predictors in lasso regression and related problems, for computational efficiency. El Ghaoui and his colleagues have propose 'SAFE' rules, based on univariate inner products between each predictor and the outcome, which guarantee that a coefficient will be 0 in the solution vector. This provides a reduction in the number of variables that need to be entered into the optimization. We propose strong rules that are very simple and yet screen out far more predictors than the SAFE rules. This great practical improvement comes at a price: the strong rules are not foolproof and can mistakenly discard active predictors, i.e. predictors that have non-zero coefficients in the solution. We therefore combine them with simple checks of the Karush-Kuhn-Tucker conditions to ensure that the exact solution to the convex problem is delivered. Of course, any (approximate) screening method can be combined with the Karush-Kuhn-Tucker, conditions to ensure the exact solution; the strength of the strong rules lies in the fact that, in practice, they discard a very large number of the inactive predictors and almost never commit mistakes. We also derive conditions under which they are foolproof. Strong rules provide substantial savings in computational time for a variety of statistical optimization problems.},
author = {Tibshirani, Robert and Bien, Jacob and Friedman, Jerome and Hastie, Trevor and Simon, Noah and Taylor, Jonathan and Tibshirani, Ryan J},
doi = {10.1111/j.1467-9868.2011.01004.x},
file = {:home/privef/Bureau/thesis-celiac/articles/Tibshirani et al. - 2012 - Strong rules for discarding predictors in lasso-type problems.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society. Series B, Statistical methodology},
month = {mar},
number = {2},
pages = {245--266},
pmid = {25506256},
title = {{Strong rules for discarding predictors in lasso-type problems.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4262615{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {74},
year = {2012}
}

@article{Zeng2017,
abstract = {Penalized regression models such as the lasso have been extensively applied to analyzing high-dimensional data sets. However, due to memory limitations, existing R packages like glmnet and ncvreg are not capable of fitting lasso-type models for ultrahigh-dimensional, multi-gigabyte data sets that are increasingly seen in many areas such as genetics, genomics, biomedical imaging, and high-frequency finance. In this research, we implement an R package called biglasso that tackles this challenge. biglasso utilizes memory-mapped files to store the massive data on the disk, only reading data into memory when necessary during model fitting, and is thus able to handle out-of-core computation seamlessly. Moreover, it's equipped with newly proposed, more efficient feature screening rules, which substantially accelerate the computation. Benchmarking experiments show that our biglasso package, as compared to existing popular ones like glmnet, is much more memory- and computation-efficient. We further analyze a 31 GB real data set on a laptop with only 16 GB RAM to demonstrate the out-of-core computation capability of biglasso in analyzing massive data sets that cannot be accommodated by existing R packages.},
archivePrefix = {arXiv},
arxivId = {1701.05936},
author = {Zeng, Yaohui and Breheny, Patrick},
eprint = {1701.05936},
file = {:home/privef/Bureau/thesis-celiac/articles/Zeng, Breheny - 2017 - The biglasso Package A Memory- and Computation-Efficient Solver for Lasso Model Fitting with Big Data in R.pdf:pdf},
month = {jan},
title = {{The biglasso Package: A Memory- and Computation-Efficient Solver for Lasso Model Fitting with Big Data in R}},
url = {http://arxiv.org/abs/1701.05936},
year = {2017}
}
